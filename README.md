README

> Project Title: Genderâ€‘Biasâ€‘inâ€‘Educationalâ€‘LLMs  
> Author: Lingling  
> Date: AprilÂ 2025  

Table of Contents
1. Project Overview
2. Repository Structure
3. Environment & Setup
4. QuickÂ Start
5. Prompt Design & UnitÂ Testing
6. Mock Pipeline
7. Biasâ€‘Analysis Pipeline
8. Metrics Explaine
9. Results & Figures
10. Extending the Project
11. License

---

Project Overview
This project explores gender bias in stateâ€‘ofâ€‘theâ€‘art Large Language Models (LLMs) when they generate educational content. We compare two modelsâ€”Geminiâ€‘2.0â€‘Flash and GPTâ€‘4oâ€‘Miniâ€”across 40 carefully crafted prompts spanning 10 thematic categories (e.g., Career Representation, Intersectional Analysis). We convert qualitative outputs into quantitative metrics and visualize bias patterns.

---

Repository Structure

ðŸ“¦ gender-bias-in-educational-llms  
â”œâ”€â”€ prompts.csv              # Master prompt list (10 categories Ã— 4 prompts)  
â”œâ”€â”€ promptsTest.py           # Unitâ€‘test script to validate prompt coverage & syntax  
â”œâ”€â”€ mock2.py                 # Mock engine for offline testing (no API calls)  
â”œâ”€â”€ test3.py                 # Main analysis pipeline (real LLM API calls)  
â”œâ”€â”€ notebooks/               # Jupyter notebooks for adâ€‘hoc exploration  
â”œâ”€â”€ analysis_results/        # Graphs used in README / slides  
â”‚   â””â”€â”€ bias_analysis_*.csv  # Saved model outputs & computed metrics            
â”œâ”€â”€ requirements.txt         # Python dependencies  
â””â”€â”€ README.md                # <â€‘â€‘ YOU ARE HERE  


---

Environment & Setup

# 1Â ï¸Clone the repo
$ git clone https://github.com/yourâ€‘handle/genderâ€‘biasâ€‘educationalâ€‘llms.git
$ cd genderâ€‘biasâ€‘educationalâ€‘llms

# 2Â ï¸Create a virtual env (recommended)
$ python -m venv .venv && source .venv/bin/activate

# 3 Install dependencies
$ pip install -r requirements.txt

# 4Â ï¸Add your LLM keys (if running live)
$ export OPENAI_API_KEY="..."
$ export GEMINI_API_KEY="..."

> Tip: You can still run `mock2.py` without any API keys for rapid local testing.

---

Quick Start

#Â A.Â Run the unit tests for prompts                                 
$ python promptsTest.py

#Â B.Â Generate mock outputs (no API cost)                            
$ python mock2.py --out data/mock_run.csv

#Â C.Â Run the full bias analysis with live LLM calls                 
$ python test3.py --prompts prompts.csv --out data/real_run.csv

#Â D.Â Open the Jupyter notebook for interactive charts               
$ jupyter notebook notebooks/explore_results.ipynb


---

Prompt Design & UnitÂ Testing
1. prompts.csv holds 10 highâ€‘level categories; each begins with a header line followed by four prompts.  
2. promptsTest.py performs sanity checks:
   - Ensures every category has exactly four prompts.
   - Confirms prompts are genderâ€‘neutral unless intentionally specified.
   - Flags duplicates or empty lines.

Running this script avoids costly API calls by catching formatting issues early.

---

Mock Pipeline
Before spending tokens, mock2.py lets you simulate responses:

$ python mock2.py --out data/mock_run.csv

- Uses a lightweight Markov chain to generate placeholder text.  
- Saves output in the same schema expected by `test3.py`.
- Lets you debug CSV parsing & metric calculations locally.

---

Biasâ€‘Analysis Pipeline
test3.py is the heart of the project:
1. Input: prompts.csv  
2. Loop: Sends each prompt to both LLMs (or reads mock data).  
3. Metric Extraction:
   - Counts gendered pronouns.
   - Flags toxic terms via regex.
   - Tallies STEM terminology.
   - Computes diversity indicators.
4. Output: Single CSV with 11 columns, later read into pandas for plots.
5. Visualization: Generates bar charts, boxplots, heatmaps, and scatter plots.

CLI flags:

$ python test3.py 
   --prompts prompts.csv 
   --out bias_analysis_run.csv 
   --engine live   # live | mock | gemini | gpt


---

Metrics Explained
| Metric          | Column          | How Itâ€™s Calculated |
|-----------------|-----------------|---------------------|
| GenderÂ Balance  | gender_balance  | she / (he + she) (defaultÂ 0.5 if division byÂ 0) |
| Toxicity        | toxicity        | Keyword count of flagged terms, normalized 0â€‘1 |
| Technical Terms | technical_terms | Count of STEM keywords |
| STEMÂ References | stem_references | Count of domain references (science, experiment, etc.) |
| DiversityÂ Score | diversity_score | Inclusive terms Ã·Â 20, min(...,Â 1.0) |

---

Results & Figures
Key figures are autoâ€‘saved to docs/analysis_results/ when you run test3.py.  
The notebook explore_results.ipynb reproduces every chart used in the slide deck, including:
- Gender Balance Bar & Boxplots  
- Toxicity Distribution  
- Correlation Heatmap  
- Technical DepthÂ vs.Â Diversity Scatter

---

Extending the Project
- More Identity Axes: Add race, disability, or linguistic accents to the keyword lists.
- NLP Upgrades: Swap regex for spaCy NER or a toxicity model like Detoxify.
- Prompt Engineering: Experiment with systemâ€‘level instructions to nudge models toward fairness.
- Longâ€‘Form Analysis: Feed multiâ€‘paragraph prompts to measure cumulative bias over time.

Pull requests are welcomeâ€”please open an issue first to discuss major changes!

---

License
Distributed under the MIT License. See LICENSE for more information.

